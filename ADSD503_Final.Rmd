---
title: "ADS503 Final Project"
output:
  html_document:
    df_print: paged
---

## Data preprocessing

```{r}
## Load and inspect data 

# Load dataset with NA encoded as '?'
df <- read.csv("~/Downloads/heart+disease/processed.cleveland.data", 
               header = FALSE, na.strings = "?")

# Assign column names based on UCI documentation
colnames(df) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg",
                  "thalach", "exang", "oldpeak", "slope", "ca", "thal", "target")

# Identify numeric columns
numeric_cols <- sapply(df, is.numeric)

# Convert target to numeric
df$target <- as.numeric(as.character(df$target))

# Scale numeric variables
df[numeric_cols] <- scale(df[numeric_cols])

# View structure and summary
str(df)
summary(df)
```

### Handle missing data

```{r}
# Count missing values per column
sapply(df, function(x) sum(is.na(x)))

# Define mode imputation function
impute_mode <- function(x) {
  ux <- unique(x[!is.na(x)])
  ux[which.max(tabulate(match(x, ux)))]
}

# Impute 'ca' and 'thal' (categorical variables with missing values)
df$ca <- ifelse(is.na(df$ca), impute_mode(df$ca), df$ca)
df$thal <- ifelse(is.na(df$thal), impute_mode(df$thal), df$thal)
```

```{r}
# Binarize the 'target' variable: 0 = No Disease, 1â€“4 = Disease
df$target <- ifelse(df$target == 0, 0, 1)
df$target <- factor(df$target, levels = c(0, 1),
                    labels = c("No_Disease", "Disease"))

print(table(df$target))
```

```{r}
# Normalize numerical features

library(caret)

# Identify numeric columns
num_vars <- c("age", "trestbps", "chol", "thalach", "oldpeak")

# Apply standardization (center + scale)
preproc <- preProcess(df[, num_vars], method = c("center", "scale"))
df[, num_vars] <- predict(preproc, df[, num_vars])

# Confirm structure and summary statistics post-processing
str(df)
summary(df)
```

## Exploratory Data Analysis (EDA)

```{r}
summary(df)
str(df)
```

```{r}
# Missing values visualization
library(VIM)
aggr(df, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE)
```

```{r}
# Distributions of numeric variables
library(ggplot2)
ggplot(df, aes(x = age)) + 
  geom_histogram(bins = 30, fill = "steelblue", color = "white")
```

```{r}
# Distribution of key numeric variables
ggplot(df, aes(x = age)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  labs(title = "Age Distribution", x = "Age", y = "Count")

ggplot(df, aes(x = chol)) +
  geom_histogram(bins = 30, fill = "darkgreen", color = "white") +
  labs(title = "Cholesterol Distribution", x = "Cholesterol", y = "Count")

ggplot(df, aes(x = thalach)) +
  geom_histogram(bins = 30, fill = "purple", color = "white") +
  labs(title = "Max Heart Rate Distribution", x = "thalach", y = "Count")

# Target variable distribution
table(df$target)
ggplot(df, aes(x = factor(target))) +
  geom_bar(fill = "coral") +
  labs(title = "Heart Disease Outcome Distribution", x = "Target (0 = No, 1 = Yes)", y = "Count")

# Boxplots of numeric variables by target
ggplot(df, aes(x = factor(target), y = age)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Age by Heart Disease Status", x = "Target", y = "Age")

ggplot(df, aes(x = factor(target), y = thalach)) +
  geom_boxplot(fill = "lightpink") +
  labs(title = "Max Heart Rate by Heart Disease Status", x = "Target", y = "thalach")

# Correlation heatmap of numeric variables
numeric_vars <- df %>%
  select_if(is.numeric)

cor_matrix <- cor(numeric_vars, use = "complete.obs")
corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.8)
```

## Train/Test Split and Model Building

```{r}
library(caret)

# Load dataset
df <- read.csv("~/Downloads/heart+disease/processed.cleveland.data", 
               header = FALSE, na.strings = "?")

# Assign column names
colnames(df) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg",
                  "thalach", "exang", "oldpeak", "slope", "ca", "thal", "target")

# Drop rows with missing values
df <- df[!is.na(df$target), ]

# inarize the target (must be done BEFORE scaling)
df$target <- as.numeric(as.character(df$target))
df$target <- ifelse(df$target == 0, 0, 1)

# Convert target to factor
df$target <- factor(df$target, levels = c(0, 1), labels = c("No Disease", "Disease"))

# Scale numeric predictors (excluding the target)
predictor_cols <- setdiff(names(df), "target")
df[predictor_cols] <- scale(df[predictor_cols])

# Stratified train-test split
set.seed(123)
train_index <- createDataPartition(df$target, p = 0.7, list = FALSE)
train <- df[train_index, ]
test <- df[-train_index, ]

# Confirm class distributions
cat("Training set class distribution:\n")
print(prop.table(table(train$target)))

cat("\nTesting set class distribution:\n")
print(prop.table(table(test$target)))
```

### Logisitc regression model

```{r}
# Fit logistic regression model
log_model <- glm(target ~ ., data = train, family = binomial)

# Predict probabilities on the test set
log_probs <- predict(log_model, newdata = test, type = "response")

# Convert probabilities to class predictions (threshold = 0.5)
log_preds <- ifelse(log_probs > 0.5, "1", "0")

# Ensure both predicted and actual values are factors with the same levels
log_preds <- factor(log_preds, levels = c("0", "1"))
actual <- factor(test$target, levels = c("0", "1"))

# Generate confusion matrix
library(caret)
confusionMatrix(log_preds, actual)
```

### Decision tree model

```{r}
### Decision Tree Model

# Load required package
library(rpart)

# Fit decision tree model
tree_model <- rpart(target ~ ., data = train, method = "class")

# Predict class labels on test set
tree_preds <- predict(tree_model, newdata = test, type = "class")

# Convert predicted and actual labels to factors with consistent levels
tree_preds <- factor(tree_preds, levels = c("0", "1"))
actual <- factor(test$target, levels = c("0", "1"))

# Evaluate using confusion matrix
library(caret)
confusionMatrix(tree_preds, actual)
```

### Random Forest Model

```{r}
# Random Forest
rf_model <- randomForest(target ~ ., data = train, ntree = 100, importance = TRUE)

# Predict
rf_preds <- predict(rf_model, test)

# Evaluation
confusionMatrix(rf_preds, test$target)

# Variable importance plot
varImpPlot(rf_model)
```
### Support Vector Machine

```{r}
# Load required libraries
library(e1071)
library(caret)

# Ensure target is a factor
df$target <- as.factor(df$target)

# Check for any missing values in full dataset
colSums(is.na(df))

# Drop rows with missing values (safe for modeling)
df_clean <- na.omit(df)

# Re-split cleaned dataset into training and testing
set.seed(123)
splitIndex <- createDataPartition(df_clean$target, p = 0.7, list = FALSE)
train <- df_clean[splitIndex, ]
test <- df_clean[-splitIndex, ]

# Confirm no missing values
stopifnot(!any(is.na(train)), !any(is.na(test)))

# Train linear SVM
svm_model <- svm(target ~ ., data = train, kernel = "linear", probability = TRUE)

# Predict class labels
svm_preds <- predict(svm_model, newdata = test)

# Align factor levels
svm_preds <- factor(svm_preds, levels = levels(test$target))
test_target <- factor(test$target, levels = levels(test$target))

# Ensure lengths match
stopifnot(length(svm_preds) == length(test_target))

# Evaluate
confusionMatrix(svm_preds, test_target)
```

### k-Nearest Neighbours

```{r}
# Load necessary library
library(class)
library(caret)

# Prepare data: separate predictors and target
train_x <- train[, setdiff(names(train), "target")]
test_x <- test[, setdiff(names(test), "target")]
train_y <- train$target
test_y <- test$target

# Convert predictors to numeric
train_x <- data.frame(lapply(train_x, as.numeric))
test_x <- data.frame(lapply(test_x, as.numeric))

# Convert target to factor
train_y <- factor(train_y, levels = c("No_Disease", "Disease"))
test_y <- factor(test_y, levels = c("No_Disease", "Disease"))

# Run k-NN classification (k = 5)
knn_preds <- knn(train = train_x, test = test_x, cl = train_y, k = 5)

# Evaluate model
confusionMatrix(knn_preds, test_y)
```

```{r}
# Load package
library(gbm)
library(caret)

# Ensure target is numeric: 0 = No Disease, 1 = Disease
train_gbm <- train
test_gbm <- test
train_gbm$target <- ifelse(train_gbm$target == "Disease", 1, 0)
test_gbm$target <- ifelse(test_gbm$target == "Disease", 1, 0)

# Train GBM model
set.seed(123)
gbm_model <- gbm(target ~ ., 
                 data = train_gbm, 
                 distribution = "bernoulli", 
                 n.trees = 100, 
                 interaction.depth = 3, 
                 cv.folds = 5, 
                 verbose = FALSE)

# Predict probabilities on test set
gbm_probs <- predict(gbm_model, test_gbm, n.trees = 100, type = "response")

# Convert probabilities to binary class predictions
gbm_preds <- ifelse(gbm_probs > 0.5, 1, 0)
gbm_preds <- factor(gbm_preds, levels = c(0, 1))
actual <- factor(test_gbm$target, levels = c(0, 1))

# Evaluate model performance
confusionMatrix(gbm_preds, actual)
```
